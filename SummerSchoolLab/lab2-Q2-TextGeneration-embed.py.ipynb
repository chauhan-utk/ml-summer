{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making an RNN  learn to Generate English Text ###\n",
    "- an RNN  is trained in seq2seq manner to make it learn to generate text\n",
    "- with lots of text fed to the network it models the language\n",
    "- The text corpus is split into chunks of fixed length \n",
    "- Each character is represented using an index\n",
    "- it learns to model the conditional probability of having a character as next character, given its previous N characters\n",
    "- This code does the unrolling of RNN explicitly using a for loop, to demosntrate how hidden state (output of hidden layer) is carrried forward to the next time-step \n",
    "\n",
    "\n",
    "<b>Acknowledgement :</b>- This code is almost completely copied from here https://gist.github.com/michaelklachko?direction=desc&sort=updated . "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import time, math\n",
    " \n",
    "torch.backends.cudnn.enabled = True\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    print ('CUDA is available')\n",
    "#use_cuda=False   #uncomment this if you dont want to use cuda variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training RNN on The Complete Sherlock Holmes.\n",
      "\n",
      "\n",
      "File length: 3867934 characters\n",
      "Unique characters: 52\n",
      "\n",
      "Unique characters: ['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '?', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "('no of uniq chars', 52)\n"
     ]
    }
   ],
   "source": [
    "printable = string.printable\n",
    " \n",
    "#Input text is available here: https://sherlock-holm.es/stories/plain-text/cano.txt\n",
    "text = open('../../../data/lab2/sh.txt', 'r').read().lower()\n",
    "\n",
    "#with open(\"./sh.txt\", \"w\") as f:\n",
    "#    f.write(text)\n",
    "\n",
    "## remove non printable chars and other unnecessary punctuations\n",
    "pruned_text = ''\n",
    " \n",
    "for c in text:\n",
    "\tif c in printable and c not in '{}[]&_':\n",
    "\t\tpruned_text += c\n",
    " \n",
    "text = pruned_text\t\t  \n",
    "file_len = len(text)\n",
    "alphabet = sorted(list(set(text)))\n",
    "n_chars = len(alphabet)\n",
    "\n",
    "print \"\\nTraining RNN on The Complete Sherlock Holmes.\\n\"\t\t \n",
    "print \"\\nFile length: {:d} characters\\nUnique characters: {:d}\".format(file_len, n_chars)\n",
    "print \"\\nUnique characters:\", alphabet\t\t \n",
    "print ('no of uniq chars', n_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~ \\t\\n\\r\\x0b\\x0c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "printable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def time_since(since):\n",
    "\ts = time.time() - since\n",
    "\tm = math.floor(s / 60)\n",
    "\ts -= m * 60\n",
    "\treturn '%dm %ds' % (m, s)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_chunk():\n",
    "    start = random.randint(0, file_len - chunk_len)\n",
    "    end = start + chunk_len + 1\n",
    "    return text[start:end]\n",
    "\n",
    "def chunk_vector(chunk):\n",
    "    vector = torch.zeros(len(chunk)).long()\n",
    "    for i, c in enumerate(chunk):\n",
    "        vector[i] = alphabet.index(c)  #construct ASCII vector for chunk, one number per character\n",
    "    return Variable(vector.cuda(), requires_grad=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_training_batch():\n",
    "    inputs = []\n",
    "    targets = []\n",
    "    #construct list of input vectors (chunk_len):\n",
    "    for b in range(batch_size):    \n",
    "        chunk = random_chunk()\n",
    "        # input will be all sequence except the last\n",
    "        inp = chunk_vector(chunk[:-1])\n",
    "        # target will be all sequence except the first\n",
    "        target = chunk_vector(chunk[1:])\n",
    "        inputs.append(inp)\n",
    "        targets.append(target)\n",
    "    #construct batches from lists (chunk_len, batch_size):\n",
    "    #need .view to handle batch_size=1\n",
    "    #need .contiguous to allow .view later\n",
    "    inp = torch.cat(inputs, 0).view(batch_size, chunk_len).t().contiguous()\n",
    "    target = torch.cat(targets, 0).view(batch_size, chunk_len).t().contiguous()\n",
    "    return inp, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = []\n",
    "input = []\n",
    "for b in range(batch_size):\n",
    "    chunk = random_chunk()\n",
    "    target.append(chunk_vector(chunk[1:]))\n",
    "    input.append(chunk_vector(chunk[:-1]))\n",
    "\n",
    "inp = torch.cat(input, 0).view(batch_size, chunk_len).t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 128])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.t().size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "blah = torch.cat(input, 0).view(batch_size, chunk_len).t().contiguous()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling language modelling as a sequence to sequence learning ###\n",
    "![char-rnn seq2seq](charrnnembed.png)\n",
    "\n",
    "\n",
    "- Input and Target sequences are sequences of characters one shifted in postion\n",
    "- For example if your corpus is \"cvit summer school\" and your chunk_len=4,\n",
    "    - then the first chunk =\"cvit\" . \n",
    "    - Input sequence will be \"cvi\" and \n",
    "    - target is \"vit\"\n",
    " \n",
    "    \n",
    ".\n",
    "\n",
    "```haskell\n",
    "Network.forward :: x(t), h(t-1) -> y(t), h(t)\n",
    "```\n",
    "\n",
    "Inorder to better understand by manipulating the hidden states, we're building the module so that we can see the hidden state being used explicitly. \n",
    "\n",
    "We're using a `GRU`, you can substitute it with an `RNN` or an `LSTM`, with the required parameters. For an `LSTM`, you'll have to additionally manipulate the cell state in the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, n_layers):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.encoder = nn.Embedding(input_size, hidden_size) #first arg is dictionary size\n",
    "        self.GRU = nn.GRU(hidden_size, hidden_size, n_layers)  #(input_size, hidden_size, n_layers)\n",
    "        self.decoder = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, input, hidden, batch_size):\n",
    "        #expand input vector length from single number to hidden_size vector\n",
    "        #Input: LongTensor (batch, seq_len)\n",
    "        #Output: (batch, seq_len, hidden_size)\n",
    "        input = self.encoder(input.view(batch_size, seq_len)) \n",
    "        #need to reshape Input to (seq_len, batch, hidden_size)\n",
    "        input = input.permute(1, 0, 2)\n",
    "        #Hidden (num_layers * num_directions, batch, hidden_size), num_directions = 2 for BiRNN\n",
    "        #Output (seq_len, batch, hidden_size * num_directions)\n",
    "        output, hidden = self.GRU(input, hidden) \n",
    "        #output, hidden = self.GRU(input.view(seq_len, batch_size, hidden_size), hidden) \n",
    "        #Output becomes (batch, hidden_size * num_directions), seq_len=1 (single char)\n",
    "        output = self.decoder(output.view(batch_size, hidden_size))  \n",
    "        #now the output is (batch_size, output_size)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        #Hidden (num_layers * num_directions, batch, hidden_size), num_directions = 2 for BiRNN\n",
    "        return Variable(torch.randn(self.n_layers, batch_size, self.hidden_size).cuda())\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model parameters:\n",
      "\n",
      "n_batches: 200\n",
      "batch_size: 16\n",
      "chunk_len: 128\n",
      "hidden_size: 256\n",
      "n_layers: 2\n",
      "LR: 0.0050\n",
      "\n",
      "\n",
      "Random chunk of text:\n",
      "\n",
      "'t look to\n",
      "     me for a penny--not a penny! you understand that, mr. detective! i am\n",
      "     all the family that this young man has \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTake input, target pairs of chunks (target is shifted forward by a single character)\\nconvert them into chunk vectors\\nfor each char pair (i, t) in chunk vectors (input, target), create embeddings with dim = hidden_size\\nfeed input char vectors to GRU model, and compute error = output - target\\nupdate weights after going through all chars in the chunk\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seq_len = 1        #each character is encoded as a single integer\n",
    "chunk_len = 128    #number of characters in a single text sample\n",
    "batch_size = 16   #number of text samples in a batch\n",
    "n_batches = 200   #size of training dataset (total number of batches)\n",
    "hidden_size = 256  #width of model\n",
    "n_layers = 2      #depth of model\n",
    "LR = 0.005         #learning rate\n",
    "\n",
    "#net = RNN(n_chars, hidden_size, n_chars, n_layers).cuda()\n",
    "net = RNN(n_chars, hidden_size, n_chars, n_layers).cuda()\n",
    "optim = torch.optim.Adam(net.parameters(), LR)\n",
    "cost = nn.CrossEntropyLoss().cuda()  \n",
    "\n",
    "print \"\\nModel parameters:\\n\"\n",
    "print \"n_batches: {:d}\\nbatch_size: {:d}\\nchunk_len: {:d}\\nhidden_size: {:d}\\nn_layers: {:d}\\nLR: {:.4f}\\n\".format(n_batches, batch_size, chunk_len, hidden_size, n_layers, LR)\n",
    "print \"\\nRandom chunk of text:\\n\\n\", random_chunk(), '\\n'\n",
    "    \n",
    "\"\"\"\n",
    "Take input, target pairs of chunks (target is shifted forward by a single character)\n",
    "convert them into chunk vectors\n",
    "for each char pair (i, t) in chunk vectors (input, target), create embeddings with dim = hidden_size\n",
    "feed input char vectors to GRU model, and compute error = output - target\n",
    "update weights after going through all chars in the chunk\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(prime_str = 'a', predict_len = 100, temp = 0.8, batch_size = 1):\n",
    "    hidden = net.init_hidden(batch_size) \n",
    "    prime_input = chunk_vector(prime_str)\n",
    "    predicted = prime_str\n",
    "    \n",
    "    for i in range(len(prime_str)-1):\n",
    "        _, hidden = net(prime_input[i], hidden, batch_size)\n",
    "     \n",
    "    inp = prime_input[-1]\n",
    "    \n",
    "    for i in range(predict_len):\n",
    "        output, hidden = net(inp, hidden, batch_size)\n",
    "        output_dist = output.data.view(-1).div(temp).exp()  \n",
    "        top_i = torch.multinomial(output_dist, 1)[0]\n",
    "        \n",
    "        predicted_char = alphabet[top_i]\n",
    "        predicted +=  predicted_char\n",
    "        inp = chunk_vector(predicted_char)\n",
    "\n",
    "    return predicted\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "whw3898b)\n",
      "c\n",
      "?tb?ue:o5h tee0ea`s5m3'z/t)u5us.wvc:72y*86cx8q8blutingt'1?(fe.80.p*4\" a\n",
      "cx'55(xt3oqfbanigw \n",
      "\n",
      "[0m 2s (0 / 200) loss: 3.9678]\n",
      "\n",
      "\n",
      "Sample output:\n",
      "\n",
      "whiten?\"\n",
      "     \"it and a stoo. he door. the rungs of of he with a crack at\n",
      "    to the the his life. ell \n",
      "\n",
      "[0m 34s (100 / 200) loss: 1.6818]\n"
     ]
    }
   ],
   "source": [
    "\n",
    " \n",
    "start = time.time()\n",
    "\n",
    "training_set = []\n",
    "\n",
    "for i in range(n_batches):\n",
    "    training_set.append((random_training_batch()))\n",
    "\n",
    "i = 0    \n",
    "for inp, target in training_set:\n",
    "    #re-init hidden outputs, zero grads, zero loss:\n",
    "    hidden = net.init_hidden(batch_size)\n",
    "    net.zero_grad()\n",
    "    loss = 0        \n",
    "    #for each char in a chunk:\n",
    "    #compute output, error, loss:\n",
    "    for c, t in zip(inp, target):\n",
    "        output, hidden = net(c, hidden, batch_size)\n",
    "        loss += cost(output, t)\n",
    "    #calculate gradients, update weights:\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print \"\\n\\nSample output:\\n\"\n",
    "        print evaluate('wh', 100, 0.8), '\\n'\n",
    "        print('[%s (%d / %d) loss: %.4f]' % (time_since(start), i, n_batches, loss.data[0] / chunk_len))\n",
    "\n",
    "    i += 1      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1 ###\n",
    "1. Why do you have to take the hidden state from the network each time and pass it along with the next input\n",
    "2. For how long does the hidden state is carried forward during training. \n",
    "    - A. it is carried forward from one time step to another, within a sequence. But not from last time step in a sequence to the first timestep of the next sequencce\n",
    "    - B. Not just across time steps within a sequence it is carried forward from one sequence to another\n",
    "    - C. It is carried forward all throughout the training. \n",
    "3. For what value of T is the sampling equivalent to doing an argmax (or picking the most probable label) sampling\n",
    "4. Vary the value of T and see how the text generated varies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 2###\n",
    "\n",
    "1. In the above code the learning is modelled as seq2seq problem. Your input is a sequence of characters and target is another sequence of characters. Which essentially means you have a target at each time step of the sequence. But this problem of text generation can also be modelled as a sequence to one problem. Then input would be sequence and target is just the next_char in the sequence. Can you modify the code to do this? ( Remember that since it is sequence to one, the output of the hidden layer need to be fed to the output layer only at the last time step)\n",
    "\n",
    "\n",
    "4. Try using MSE loss for the above problem. How does the network converge with an MSE loss? Why did MSE perfrom poorer or better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
