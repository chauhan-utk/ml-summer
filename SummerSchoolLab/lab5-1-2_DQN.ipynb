{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Networks\n",
    "\n",
    "In this lab we will train a Deep Q-Network (DQN) agent to play Pong task (Atari game) from OpenAI gym.\n",
    "\n",
    "## 1. Task\n",
    "The game of Pong is an excellent example of a simple RL task. In the ATARI 2600 version we’ll use you play as one of the paddles (the other is controlled by a decent AI) and you have to bounce the ball past the other player (I don’t really have to explain Pong, right?). On the low level the game works as follows: we receive an image frame (a 210x160x3 byte array (integers from 0 to 255 giving pixel values)) and we get to decide if we want to move the paddle UP or DOWN (i.e. a binary choice). After every single choice the game simulator executes the action and gives us a reward: Either a +1 reward if the ball went past the opponent, a -1 reward if we missed the ball, or 0 otherwise. And of course, our goal is to move the paddle so that we get lots of reward.\n",
    "\n",
    "<img src=images/pong.gif>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. From Q-Network to Deep Q-Network\n",
    "\n",
    "We saw that ordinary Q-network was able to barely perform as well as the Q-Table in a simple game environment, Deep Q-Networks are much more capable. In order to transform an ordinary Q-Network into a DQN we will be making the following improvements:\n",
    "- Going from a single-layer network to a multi-layer convolutional network.\n",
    "- Implementing Experience Replay, which will allow our network to train itself using stored memories from it’s experience.\n",
    "- Utilizing a second “target” network, which we will use to compute target Q-values during our updates.\n",
    "\n",
    "It was these three innovations that allowed the [Google DeepMind team to achieve superhuman performance on dozens of Atari games using their DQN agent](http://www.davidqiu.com:8888/research/nature14236.pdf). We will be walking through each individual improvement.\n",
    "\n",
    "### Addition 1: Convolutional Layers\n",
    "Since our agent is going to be learning to play video games, it has to be able to make sense of the game’s screen output in a way that is at least similar to how humans or other intelligent animals are able to. Instead of considering each pixel independently, convolutional layers allow us to consider regions of an image, and maintain spatial relationships between the objects on the screen as we send information up to higher levels of the network. \n",
    "\n",
    "### Addition 2: Experience Replay\n",
    "The second major addition to make DQNs work is Experience Replay. The basic idea is that by storing an agent’s experiences, and then randomly drawing batches of them to train the network, we can more robustly learn to perform well in the task. By keeping the experiences we draw random, we prevent the network from only learning about what it is immediately doing in the environment, and allow it to learn from a more varied array of past experiences. Each of these experiences are stored as a tuple of $\\text{<state, action, reward, next_state>}$. The Experience Replay buffer stores a fixed number of recent memories, and as new ones come in, old ones are removed. When the time comes to train, we simply draw a uniform batch of random memories from the buffer, and train our network with them. For our DQN, we have a helper function named `ReplayBuffer` which implements this.\n",
    "\n",
    "### Addition 3: Separate Target Network\n",
    "The third major addition to the DQN that makes it unique is the utilization of a second network during the training procedure. This second network is used to generate the target-Q values that will be used to compute the loss for every action during training. Why not use just use one network for both estimations? The issue is that at every step of training, the Q-network’s values shift, and if we are using a constantly shifting set of values to adjust our network values, then the value estimations can easily spiral out of control. The network can become destabilized by falling into feedback loops between the target and estimated Q-values. In order to mitigate that risk, the target network’s weights are fixed, and only periodically or slowly updated to the primary Q-networks values. In this way training can proceed in a more stable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from itertools import count\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import gym.spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "from utils.gym import get_env, get_wrapper_by_name\n",
    "from utils.schedule import ConstantSchedule, PiecewiseSchedule, LinearSchedule\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###########################\n",
    "# Define Global Variables #\n",
    "###########################\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "GAMMA = 0.99\n",
    "REPLAY_BUFFER_SIZE = 1000000\n",
    "LEARNING_STARTS = 50000\n",
    "LEARNING_FREQ = 4\n",
    "FRAME_HISTORY_LEN = 10\n",
    "TARGET_UPDATE_FREQ = 10000\n",
    "LEARNING_RATE = 0.00025\n",
    "ALPHA = 0.95\n",
    "EPS = 0.01\n",
    "PONG = 3\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "dtype = torch.cuda.FloatTensor if torch.cuda.is_available() else torch.FloatTensor\n",
    "\n",
    "class Variable(autograd.Variable):\n",
    "    def __init__(self, data, *args, **kwargs):\n",
    "        if USE_CUDA:\n",
    "            data = data.cuda()\n",
    "        super(Variable, self).__init__(data, *args, **kwargs)\n",
    "\n",
    "\"\"\"\n",
    "    OptimizerSpec containing following attributes\n",
    "        constructor: The optimizer constructor ex: RMSprop\n",
    "        kwargs: {Dict} arguments for constructing optimizer\n",
    "\"\"\"\n",
    "OptimizerSpec = namedtuple(\"OptimizerSpec\", [\"constructor\", \"kwargs\"])\n",
    "\n",
    "Statistic = {\n",
    "    \"mean_episode_rewards\": [],\n",
    "    \"best_mean_episode_rewards\": []\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##############\n",
    "# Define DQN #\n",
    "##############\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, in_channels=4, num_actions=18):\n",
    "        \"\"\"\n",
    "        Initialize a deep Q-learning network as described in\n",
    "        https://storage.googleapis.com/deepmind-data/assets/papers/DeepMindNature14236Paper.pdf\n",
    "        Arguments:\n",
    "            in_channels: number of channel of input.\n",
    "                i.e The number of most recent frames stacked together as describe in the paper\n",
    "            num_actions: number of action-value to output, one-to-one correspondence to action in game.\n",
    "        \"\"\"\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, 32, kernel_size=8, stride=4)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=4, stride=2)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc4 = nn.Linear(7 * 7 * 64, 512)\n",
    "        self.fc5 = nn.Linear(512, num_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.fc4(x.view(x.size(0), -1)))\n",
    "        return self.fc5(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "## TRAINING FUNCTION ##\n",
    "#######################\n",
    "\n",
    "def dqn_learing(\n",
    "    env,\n",
    "    q_func,\n",
    "    optimizer_spec,\n",
    "    exploration,\n",
    "    stopping_criterion=None,\n",
    "    replay_buffer_size=1000000,\n",
    "    batch_size=32,\n",
    "    gamma=0.99,\n",
    "    learning_starts=50000,\n",
    "    learning_freq=4,\n",
    "    frame_history_len=10,\n",
    "    target_update_freq=10000\n",
    "    ):\n",
    "\n",
    "    \"\"\"Run Deep Q-learning algorithm.\n",
    "\n",
    "    You can specify your own convnet using q_func.\n",
    "\n",
    "    All schedules are w.r.t. total number of steps taken in the environment.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    env: gym.Env\n",
    "        gym environment to train on.\n",
    "    q_func: function\n",
    "        Model to use for computing the q function. It should accept the\n",
    "        following named arguments:\n",
    "            input_channel: int\n",
    "                number of channel of input.\n",
    "            num_actions: int\n",
    "                number of actions\n",
    "    optimizer_spec: OptimizerSpec\n",
    "        Specifying the constructor and kwargs, as well as learning rate schedule\n",
    "        for the optimizer\n",
    "    exploration: Schedule (defined in utils.schedule)\n",
    "        schedule for probability of chosing random action.\n",
    "    stopping_criterion: (env) -> bool\n",
    "        should return true when it's ok for the RL algorithm to stop.\n",
    "        takes in env and the number of steps executed so far.\n",
    "    replay_buffer_size: int\n",
    "        How many memories to store in the replay buffer.\n",
    "    batch_size: int\n",
    "        How many transitions to sample each time experience is replayed.\n",
    "    gamma: float\n",
    "        Discount Factor\n",
    "    learning_starts: int\n",
    "        After how many environment steps to start replaying experiences\n",
    "    learning_freq: int\n",
    "        How many steps of environment to take between every experience replay\n",
    "    frame_history_len: int\n",
    "        How many past frames to include as input to the model.\n",
    "    target_update_freq: int\n",
    "        How many experience replay rounds (not steps!) to perform between\n",
    "        each update to the target Q network\n",
    "    \"\"\"\n",
    "    assert type(env.observation_space) == gym.spaces.Box\n",
    "    assert type(env.action_space)      == gym.spaces.Discrete\n",
    "\n",
    "    ###############\n",
    "    # BUILD MODEL #\n",
    "    ###############\n",
    "\n",
    "    if len(env.observation_space.shape) == 1:\n",
    "        # This means we are running on low-dimensional observations (e.g. RAM)\n",
    "        input_arg = env.observation_space.shape[0]\n",
    "    else:\n",
    "        img_h, img_w, img_c = env.observation_space.shape\n",
    "        input_arg = frame_history_len * img_c\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # Construct an epilson greedy policy with given exploration schedule\n",
    "    def select_epilson_greedy_action(model, obs, t):\n",
    "        sample = random.random()\n",
    "        eps_threshold = exploration.value(t)\n",
    "        if sample > eps_threshold:\n",
    "            obs = torch.from_numpy(obs).type(dtype).unsqueeze(0) / 255.0\n",
    "            # Use volatile = True if variable is only used in inference mode, i.e. don’t save the history\n",
    "            return model(Variable(obs, volatile=True)).data.max(1)[1].cpu()\n",
    "        else:\n",
    "            return torch.IntTensor([[random.randrange(num_actions)]])\n",
    "\n",
    "    # Initialize target q function and q function\n",
    "    Q = q_func(input_arg, num_actions).type(dtype)\n",
    "    target_Q = q_func(input_arg, num_actions).type(dtype)\n",
    "\n",
    "    # Construct Q network optimizer function\n",
    "    optimizer = optimizer_spec.constructor(Q.parameters(), **optimizer_spec.kwargs)\n",
    "\n",
    "    # Construct the replay buffer\n",
    "    replay_buffer = ReplayBuffer(replay_buffer_size, frame_history_len)\n",
    "\n",
    "    ###############\n",
    "    # RUN ENV     #\n",
    "    ###############\n",
    "    num_param_updates = 0\n",
    "    mean_episode_reward = -float('nan')\n",
    "    best_mean_episode_reward = -float('inf')\n",
    "    last_obs = env.reset()\n",
    "    LOG_EVERY_N_STEPS = 10000\n",
    "\n",
    "    for t in count():\n",
    "        ### Check stopping criterion\n",
    "        if stopping_criterion is not None and stopping_criterion(env):\n",
    "            break\n",
    "\n",
    "        ### Step the env and store the transition\n",
    "        # Store lastest observation in replay memory and last_idx can be used to store action, reward, done\n",
    "        last_idx = replay_buffer.store_frame(last_obs)\n",
    "        # encode_recent_observation will take the latest observation\n",
    "        # that you pushed into the buffer and compute the corresponding\n",
    "        # input that should be given to a Q network by appending some\n",
    "        # previous frames.\n",
    "        recent_observations = replay_buffer.encode_recent_observation()\n",
    "\n",
    "        # Choose random action if not yet start learning\n",
    "        if t > learning_starts:\n",
    "            action = select_epilson_greedy_action(Q, recent_observations, t)[0, 0]\n",
    "        else:\n",
    "            action = random.randrange(num_actions)\n",
    "        # Advance one step\n",
    "        obs, reward, done, _ = env.step(action)\n",
    "        # clip rewards between -1 and 1\n",
    "        reward = max(-1.0, min(reward, 1.0))\n",
    "        # Store other info in replay memory\n",
    "        replay_buffer.store_effect(last_idx, action, reward, done)\n",
    "        # Resets the environment when reaching an episode boundary.\n",
    "        if done:\n",
    "            obs = env.reset()\n",
    "        last_obs = obs\n",
    "\n",
    "        ### Perform experience replay and train the network.\n",
    "        # Note that this is only done if the replay buffer contains enough samples\n",
    "        # for us to learn something useful -- until then, the model will not be\n",
    "        # initialized and random actions should be taken\n",
    "        if (t > learning_starts and\n",
    "                t % learning_freq == 0 and\n",
    "                replay_buffer.can_sample(batch_size)):\n",
    "            # Use the replay buffer to sample a batch of transitions\n",
    "            # Note: done_mask[i] is 1 if the next state corresponds to the end of an episode,\n",
    "            # in which case there is no Q-value at the next state; at the end of an\n",
    "            # episode, only the current state reward contributes to the target\n",
    "            obs_batch, act_batch, rew_batch, next_obs_batch, done_mask = replay_buffer.sample(batch_size)\n",
    "            # Convert numpy nd_array to torch variables for calculation\n",
    "            obs_batch = Variable(torch.from_numpy(obs_batch).type(dtype) / 255.0)\n",
    "            act_batch = Variable(torch.from_numpy(act_batch).long())\n",
    "            rew_batch = Variable(torch.from_numpy(rew_batch))\n",
    "            next_obs_batch = Variable(torch.from_numpy(next_obs_batch).type(dtype) / 255.0)\n",
    "            not_done_mask = Variable(torch.from_numpy(1 - done_mask)).type(dtype)\n",
    "\n",
    "            if USE_CUDA:\n",
    "                act_batch = act_batch.cuda()\n",
    "                rew_batch = rew_batch.cuda()\n",
    "\n",
    "            # Compute current Q value, q_func takes only state and output value for every state-action pair\n",
    "            # We choose Q based on action taken.\n",
    "            current_Q_values = Q(obs_batch).gather(1, act_batch.unsqueeze(1))\n",
    "            # Compute next Q value based on which action gives max Q values\n",
    "            # Detach variable from the current graph since we don't want gradients for next Q to propagated\n",
    "            next_max_q = target_Q(next_obs_batch).detach().max(1)[0]\n",
    "            next_Q_values = not_done_mask * next_max_q\n",
    "            # Compute the target of the current Q values\n",
    "            target_Q_values = rew_batch + (gamma * next_Q_values)\n",
    "            # Compute Bellman error\n",
    "            bellman_error = target_Q_values - current_Q_values\n",
    "            # clip the bellman error between [-1 , 1]\n",
    "            clipped_bellman_error = bellman_error.clamp(-1, 1)\n",
    "            # Note: clipped_bellman_delta * -1 will be right gradient\n",
    "            d_error = clipped_bellman_error * -1.0\n",
    "            # Clear previous gradients before backward pass\n",
    "            optimizer.zero_grad()\n",
    "            # run backward pass\n",
    "            current_Q_values.backward(d_error.data.unsqueeze(1))\n",
    "\n",
    "            # Perfom the update\n",
    "            optimizer.step()\n",
    "            num_param_updates += 1\n",
    "\n",
    "            # Periodically update the target network by Q network to target Q network\n",
    "            if num_param_updates % target_update_freq == 0:\n",
    "                target_Q.load_state_dict(Q.state_dict())\n",
    "\n",
    "        ### 4. Log progress and keep track of statistics\n",
    "        episode_rewards = get_wrapper_by_name(env, \"Monitor\").get_episode_rewards()\n",
    "        if len(episode_rewards) > 0:\n",
    "            mean_episode_reward = np.mean(episode_rewards[-100:])\n",
    "        if len(episode_rewards) > 100:\n",
    "            best_mean_episode_reward = max(best_mean_episode_reward, mean_episode_reward)\n",
    "\n",
    "        Statistic[\"mean_episode_rewards\"].append(mean_episode_reward)\n",
    "        Statistic[\"best_mean_episode_rewards\"].append(best_mean_episode_reward)\n",
    "\n",
    "        if t % LOG_EVERY_N_STEPS == 0 and t > learning_starts:\n",
    "            print(\"Timestep %d\" % (t,))\n",
    "            print(\"mean reward (100 episodes) %f\" % mean_episode_reward)\n",
    "            print(\"best mean reward %f\" % best_mean_episode_reward)\n",
    "            print(\"episodes %d\" % len(episode_rewards))\n",
    "            print(\"exploration %f\" % exploration.value(t))\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            # Dump statistics to pickle\n",
    "            # with open('statistics.pkl', 'wb') as f:\n",
    "            #     pickle.dump(Statistic, f)\n",
    "            #     print(\"Saved to %s\" % 'results/statistics.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 17:33:58,332] Making new env: PongNoFrameskip-v4\n",
      "[2017-07-14 17:33:58,721] Clearing 12 monitor files from previous run (because force=True was provided)\n",
      "[2017-07-14 17:34:01,539] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000000.mp4\n",
      "[2017-07-14 17:34:04,087] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000001.mp4\n",
      "[2017-07-14 17:34:14,503] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000008.mp4\n",
      "[2017-07-14 17:34:42,761] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000027.mp4\n",
      "[2017-07-14 17:35:59,732] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000064.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 60000\n",
      "mean reward (100 episodes) -20.307692\n",
      "best mean reward -inf\n",
      "episodes 65\n",
      "exploration 0.946000\n",
      "Timestep 70000\n",
      "mean reward (100 episodes) -20.328947\n",
      "best mean reward -inf\n",
      "episodes 76\n",
      "exploration 0.937000\n",
      "Timestep 80000\n",
      "mean reward (100 episodes) -20.337209\n",
      "best mean reward -inf\n",
      "episodes 86\n",
      "exploration 0.928000\n",
      "Timestep 90000\n",
      "mean reward (100 episodes) -20.288660\n",
      "best mean reward -inf\n",
      "episodes 97\n",
      "exploration 0.919000\n",
      "Timestep 100000\n",
      "mean reward (100 episodes) -20.340000\n",
      "best mean reward -20.300000\n",
      "episodes 108\n",
      "exploration 0.910000\n",
      "Timestep 110000\n",
      "mean reward (100 episodes) -20.380000\n",
      "best mean reward -20.300000\n",
      "episodes 119\n",
      "exploration 0.901000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 17:40:15,433] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000125.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 120000\n",
      "mean reward (100 episodes) -20.360000\n",
      "best mean reward -20.300000\n",
      "episodes 130\n",
      "exploration 0.892000\n",
      "Timestep 130000\n",
      "mean reward (100 episodes) -20.300000\n",
      "best mean reward -20.290000\n",
      "episodes 140\n",
      "exploration 0.883000\n",
      "Timestep 140000\n",
      "mean reward (100 episodes) -20.290000\n",
      "best mean reward -20.280000\n",
      "episodes 151\n",
      "exploration 0.874000\n",
      "Timestep 150000\n",
      "mean reward (100 episodes) -20.310000\n",
      "best mean reward -20.280000\n",
      "episodes 162\n",
      "exploration 0.865000\n",
      "Timestep 160000\n",
      "mean reward (100 episodes) -20.300000\n",
      "best mean reward -20.280000\n",
      "episodes 173\n",
      "exploration 0.856000\n",
      "Timestep 170000\n",
      "mean reward (100 episodes) -20.260000\n",
      "best mean reward -20.250000\n",
      "episodes 184\n",
      "exploration 0.847000\n",
      "Timestep 180000\n",
      "mean reward (100 episodes) -20.270000\n",
      "best mean reward -20.220000\n",
      "episodes 195\n",
      "exploration 0.838000\n",
      "Timestep 190000\n",
      "mean reward (100 episodes) -20.240000\n",
      "best mean reward -20.220000\n",
      "episodes 205\n",
      "exploration 0.829000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 17:46:53,832] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000216.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 200000\n",
      "mean reward (100 episodes) -20.230000\n",
      "best mean reward -20.210000\n",
      "episodes 216\n",
      "exploration 0.820000\n",
      "Timestep 210000\n",
      "mean reward (100 episodes) -20.130000\n",
      "best mean reward -20.120000\n",
      "episodes 226\n",
      "exploration 0.811000\n",
      "Timestep 220000\n",
      "mean reward (100 episodes) -20.140000\n",
      "best mean reward -20.110000\n",
      "episodes 237\n",
      "exploration 0.802000\n",
      "Timestep 230000\n",
      "mean reward (100 episodes) -20.130000\n",
      "best mean reward -20.110000\n",
      "episodes 248\n",
      "exploration 0.793000\n",
      "Timestep 240000\n",
      "mean reward (100 episodes) -20.090000\n",
      "best mean reward -20.040000\n",
      "episodes 259\n",
      "exploration 0.784000\n",
      "Timestep 250000\n",
      "mean reward (100 episodes) -20.050000\n",
      "best mean reward -20.040000\n",
      "episodes 269\n",
      "exploration 0.775000\n",
      "Timestep 260000\n",
      "mean reward (100 episodes) -20.050000\n",
      "best mean reward -20.040000\n",
      "episodes 280\n",
      "exploration 0.766000\n",
      "Timestep 270000\n",
      "mean reward (100 episodes) -20.070000\n",
      "best mean reward -20.040000\n",
      "episodes 291\n",
      "exploration 0.757000\n",
      "Timestep 280000\n",
      "mean reward (100 episodes) -20.080000\n",
      "best mean reward -20.040000\n",
      "episodes 301\n",
      "exploration 0.748000\n",
      "Timestep 290000\n",
      "mean reward (100 episodes) -20.070000\n",
      "best mean reward -20.020000\n",
      "episodes 313\n",
      "exploration 0.739000\n",
      "Timestep 300000\n",
      "mean reward (100 episodes) -20.130000\n",
      "best mean reward -20.020000\n",
      "episodes 323\n",
      "exploration 0.730000\n",
      "Timestep 310000\n",
      "mean reward (100 episodes) -20.060000\n",
      "best mean reward -20.020000\n",
      "episodes 333\n",
      "exploration 0.721000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 17:56:37,338] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000343.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 320000\n",
      "mean reward (100 episodes) -20.070000\n",
      "best mean reward -20.020000\n",
      "episodes 344\n",
      "exploration 0.712000\n",
      "Timestep 330000\n",
      "mean reward (100 episodes) -20.130000\n",
      "best mean reward -20.020000\n",
      "episodes 355\n",
      "exploration 0.703000\n",
      "Timestep 340000\n",
      "mean reward (100 episodes) -20.080000\n",
      "best mean reward -20.020000\n",
      "episodes 365\n",
      "exploration 0.694000\n",
      "Timestep 350000\n",
      "mean reward (100 episodes) -20.130000\n",
      "best mean reward -20.020000\n",
      "episodes 377\n",
      "exploration 0.685000\n",
      "Timestep 360000\n",
      "mean reward (100 episodes) -20.160000\n",
      "best mean reward -20.020000\n",
      "episodes 388\n",
      "exploration 0.676000\n",
      "Timestep 370000\n",
      "mean reward (100 episodes) -20.090000\n",
      "best mean reward -20.020000\n",
      "episodes 398\n",
      "exploration 0.667000\n",
      "Timestep 380000\n",
      "mean reward (100 episodes) -20.100000\n",
      "best mean reward -20.020000\n",
      "episodes 408\n",
      "exploration 0.658000\n",
      "Timestep 390000\n",
      "mean reward (100 episodes) -20.120000\n",
      "best mean reward -20.020000\n",
      "episodes 420\n",
      "exploration 0.649000\n",
      "Timestep 400000\n",
      "mean reward (100 episodes) -20.180000\n",
      "best mean reward -20.020000\n",
      "episodes 430\n",
      "exploration 0.640000\n",
      "Timestep 410000\n",
      "mean reward (100 episodes) -20.220000\n",
      "best mean reward -20.020000\n",
      "episodes 441\n",
      "exploration 0.631000\n",
      "Timestep 420000\n",
      "mean reward (100 episodes) -20.170000\n",
      "best mean reward -20.020000\n",
      "episodes 450\n",
      "exploration 0.622000\n",
      "Timestep 430000\n",
      "mean reward (100 episodes) -20.200000\n",
      "best mean reward -20.020000\n",
      "episodes 462\n",
      "exploration 0.613000\n",
      "Timestep 440000\n",
      "mean reward (100 episodes) -20.180000\n",
      "best mean reward -20.020000\n",
      "episodes 472\n",
      "exploration 0.604000\n",
      "Timestep 450000\n",
      "mean reward (100 episodes) -20.140000\n",
      "best mean reward -20.020000\n",
      "episodes 482\n",
      "exploration 0.595000\n",
      "Timestep 460000\n",
      "mean reward (100 episodes) -20.160000\n",
      "best mean reward -20.020000\n",
      "episodes 493\n",
      "exploration 0.586000\n",
      "Timestep 470000\n",
      "mean reward (100 episodes) -20.190000\n",
      "best mean reward -20.020000\n",
      "episodes 503\n",
      "exploration 0.577000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 18:10:06,694] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000512.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 480000\n",
      "mean reward (100 episodes) -20.150000\n",
      "best mean reward -20.020000\n",
      "episodes 513\n",
      "exploration 0.568000\n",
      "Timestep 490000\n",
      "mean reward (100 episodes) -20.070000\n",
      "best mean reward -20.020000\n",
      "episodes 522\n",
      "exploration 0.559000\n",
      "Timestep 500000\n",
      "mean reward (100 episodes) -19.990000\n",
      "best mean reward -19.990000\n",
      "episodes 531\n",
      "exploration 0.550000\n",
      "Timestep 510000\n",
      "mean reward (100 episodes) -19.950000\n",
      "best mean reward -19.920000\n",
      "episodes 540\n",
      "exploration 0.541000\n",
      "Timestep 520000\n",
      "mean reward (100 episodes) -19.890000\n",
      "best mean reward -19.890000\n",
      "episodes 550\n",
      "exploration 0.532000\n",
      "Timestep 530000\n",
      "mean reward (100 episodes) -19.820000\n",
      "best mean reward -19.820000\n",
      "episodes 559\n",
      "exploration 0.523000\n",
      "Timestep 540000\n",
      "mean reward (100 episodes) -19.790000\n",
      "best mean reward -19.780000\n",
      "episodes 568\n",
      "exploration 0.514000\n",
      "Timestep 550000\n",
      "mean reward (100 episodes) -19.610000\n",
      "best mean reward -19.610000\n",
      "episodes 576\n",
      "exploration 0.505000\n",
      "Timestep 560000\n",
      "mean reward (100 episodes) -19.640000\n",
      "best mean reward -19.600000\n",
      "episodes 586\n",
      "exploration 0.496000\n",
      "Timestep 570000\n",
      "mean reward (100 episodes) -19.500000\n",
      "best mean reward -19.500000\n",
      "episodes 595\n",
      "exploration 0.487000\n",
      "Timestep 580000\n",
      "mean reward (100 episodes) -19.500000\n",
      "best mean reward -19.500000\n",
      "episodes 605\n",
      "exploration 0.478000\n",
      "Timestep 590000\n",
      "mean reward (100 episodes) -19.490000\n",
      "best mean reward -19.460000\n",
      "episodes 614\n",
      "exploration 0.469000\n",
      "Timestep 600000\n",
      "mean reward (100 episodes) -19.450000\n",
      "best mean reward -19.440000\n",
      "episodes 623\n",
      "exploration 0.460000\n",
      "Timestep 610000\n",
      "mean reward (100 episodes) -19.420000\n",
      "best mean reward -19.420000\n",
      "episodes 632\n",
      "exploration 0.451000\n",
      "Timestep 620000\n",
      "mean reward (100 episodes) -19.390000\n",
      "best mean reward -19.390000\n",
      "episodes 640\n",
      "exploration 0.442000\n",
      "Timestep 630000\n",
      "mean reward (100 episodes) -19.360000\n",
      "best mean reward -19.340000\n",
      "episodes 649\n",
      "exploration 0.433000\n",
      "Timestep 640000\n",
      "mean reward (100 episodes) -19.310000\n",
      "best mean reward -19.310000\n",
      "episodes 657\n",
      "exploration 0.424000\n",
      "Timestep 650000\n",
      "mean reward (100 episodes) -19.280000\n",
      "best mean reward -19.280000\n",
      "episodes 666\n",
      "exploration 0.415000\n",
      "Timestep 660000\n",
      "mean reward (100 episodes) -19.100000\n",
      "best mean reward -19.090000\n",
      "episodes 673\n",
      "exploration 0.406000\n",
      "Timestep 670000\n",
      "mean reward (100 episodes) -19.090000\n",
      "best mean reward -19.080000\n",
      "episodes 681\n",
      "exploration 0.397000\n",
      "Timestep 680000\n",
      "mean reward (100 episodes) -18.990000\n",
      "best mean reward -18.970000\n",
      "episodes 689\n",
      "exploration 0.388000\n",
      "Timestep 690000\n",
      "mean reward (100 episodes) -18.920000\n",
      "best mean reward -18.920000\n",
      "episodes 697\n",
      "exploration 0.379000\n",
      "Timestep 700000\n",
      "mean reward (100 episodes) -18.780000\n",
      "best mean reward -18.780000\n",
      "episodes 704\n",
      "exploration 0.370000\n",
      "Timestep 710000\n",
      "mean reward (100 episodes) -18.690000\n",
      "best mean reward -18.690000\n",
      "episodes 711\n",
      "exploration 0.361000\n",
      "Timestep 720000\n",
      "mean reward (100 episodes) -18.530000\n",
      "best mean reward -18.530000\n",
      "episodes 718\n",
      "exploration 0.352000\n",
      "Timestep 730000\n",
      "mean reward (100 episodes) -18.450000\n",
      "best mean reward -18.450000\n",
      "episodes 725\n",
      "exploration 0.343000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 18:33:19,112] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video000729.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 740000\n",
      "mean reward (100 episodes) -18.340000\n",
      "best mean reward -18.330000\n",
      "episodes 732\n",
      "exploration 0.334000\n",
      "Timestep 750000\n",
      "mean reward (100 episodes) -18.150000\n",
      "best mean reward -18.150000\n",
      "episodes 738\n",
      "exploration 0.325000\n",
      "Timestep 760000\n",
      "mean reward (100 episodes) -17.960000\n",
      "best mean reward -17.960000\n",
      "episodes 744\n",
      "exploration 0.316000\n",
      "Timestep 770000\n",
      "mean reward (100 episodes) -17.880000\n",
      "best mean reward -17.880000\n",
      "episodes 749\n",
      "exploration 0.307000\n",
      "Timestep 780000\n",
      "mean reward (100 episodes) -17.750000\n",
      "best mean reward -17.750000\n",
      "episodes 754\n",
      "exploration 0.298000\n",
      "Timestep 790000\n",
      "mean reward (100 episodes) -17.610000\n",
      "best mean reward -17.610000\n",
      "episodes 760\n",
      "exploration 0.289000\n",
      "Timestep 800000\n",
      "mean reward (100 episodes) -17.490000\n",
      "best mean reward -17.490000\n",
      "episodes 766\n",
      "exploration 0.280000\n",
      "Timestep 810000\n",
      "mean reward (100 episodes) -17.520000\n",
      "best mean reward -17.470000\n",
      "episodes 771\n",
      "exploration 0.271000\n",
      "Timestep 820000\n",
      "mean reward (100 episodes) -17.490000\n",
      "best mean reward -17.450000\n",
      "episodes 776\n",
      "exploration 0.262000\n",
      "Timestep 830000\n",
      "mean reward (100 episodes) -17.380000\n",
      "best mean reward -17.380000\n",
      "episodes 781\n",
      "exploration 0.253000\n",
      "Timestep 840000\n",
      "mean reward (100 episodes) -17.310000\n",
      "best mean reward -17.310000\n",
      "episodes 785\n",
      "exploration 0.244000\n",
      "Timestep 850000\n",
      "mean reward (100 episodes) -17.150000\n",
      "best mean reward -17.150000\n",
      "episodes 790\n",
      "exploration 0.235000\n",
      "Timestep 860000\n",
      "mean reward (100 episodes) -17.010000\n",
      "best mean reward -17.010000\n",
      "episodes 795\n",
      "exploration 0.226000\n",
      "Timestep 870000\n",
      "mean reward (100 episodes) -16.870000\n",
      "best mean reward -16.870000\n",
      "episodes 800\n",
      "exploration 0.217000\n",
      "Timestep 880000\n",
      "mean reward (100 episodes) -16.790000\n",
      "best mean reward -16.780000\n",
      "episodes 805\n",
      "exploration 0.208000\n",
      "Timestep 890000\n",
      "mean reward (100 episodes) -16.570000\n",
      "best mean reward -16.570000\n",
      "episodes 809\n",
      "exploration 0.199000\n",
      "Timestep 900000\n",
      "mean reward (100 episodes) -16.470000\n",
      "best mean reward -16.470000\n",
      "episodes 814\n",
      "exploration 0.190000\n",
      "Timestep 910000\n",
      "mean reward (100 episodes) -16.330000\n",
      "best mean reward -16.330000\n",
      "episodes 818\n",
      "exploration 0.181000\n",
      "Timestep 920000\n",
      "mean reward (100 episodes) -16.190000\n",
      "best mean reward -16.190000\n",
      "episodes 822\n",
      "exploration 0.172000\n",
      "Timestep 930000\n",
      "mean reward (100 episodes) -16.140000\n",
      "best mean reward -16.110000\n",
      "episodes 827\n",
      "exploration 0.163000\n",
      "Timestep 940000\n",
      "mean reward (100 episodes) -16.090000\n",
      "best mean reward -16.050000\n",
      "episodes 831\n",
      "exploration 0.154000\n",
      "Timestep 950000\n",
      "mean reward (100 episodes) -15.840000\n",
      "best mean reward -15.840000\n",
      "episodes 836\n",
      "exploration 0.145000\n",
      "Timestep 960000\n",
      "mean reward (100 episodes) -15.850000\n",
      "best mean reward -15.840000\n",
      "episodes 841\n",
      "exploration 0.136000\n",
      "Timestep 970000\n",
      "mean reward (100 episodes) -15.780000\n",
      "best mean reward -15.780000\n",
      "episodes 845\n",
      "exploration 0.127000\n",
      "Timestep 980000\n",
      "mean reward (100 episodes) -15.650000\n",
      "best mean reward -15.650000\n",
      "episodes 849\n",
      "exploration 0.118000\n",
      "Timestep 990000\n",
      "mean reward (100 episodes) -15.550000\n",
      "best mean reward -15.550000\n",
      "episodes 854\n",
      "exploration 0.109000\n",
      "Timestep 1000000\n",
      "mean reward (100 episodes) -15.440000\n",
      "best mean reward -15.430000\n",
      "episodes 858\n",
      "exploration 0.100000\n",
      "Timestep 1010000\n",
      "mean reward (100 episodes) -15.340000\n",
      "best mean reward -15.340000\n",
      "episodes 861\n",
      "exploration 0.100000\n",
      "Timestep 1020000\n",
      "mean reward (100 episodes) -15.080000\n",
      "best mean reward -15.080000\n",
      "episodes 866\n",
      "exploration 0.100000\n",
      "Timestep 1030000\n",
      "mean reward (100 episodes) -14.940000\n",
      "best mean reward -14.940000\n",
      "episodes 870\n",
      "exploration 0.100000\n",
      "Timestep 1040000\n",
      "mean reward (100 episodes) -14.790000\n",
      "best mean reward -14.790000\n",
      "episodes 873\n",
      "exploration 0.100000\n",
      "Timestep 1050000\n",
      "mean reward (100 episodes) -14.500000\n",
      "best mean reward -14.500000\n",
      "episodes 877\n",
      "exploration 0.100000\n",
      "Timestep 1060000\n",
      "mean reward (100 episodes) -14.400000\n",
      "best mean reward -14.380000\n",
      "episodes 881\n",
      "exploration 0.100000\n",
      "Timestep 1070000\n",
      "mean reward (100 episodes) -14.330000\n",
      "best mean reward -14.330000\n",
      "episodes 886\n",
      "exploration 0.100000\n",
      "Timestep 1080000\n",
      "mean reward (100 episodes) -14.350000\n",
      "best mean reward -14.330000\n",
      "episodes 890\n",
      "exploration 0.100000\n",
      "Timestep 1090000\n",
      "mean reward (100 episodes) -14.210000\n",
      "best mean reward -14.180000\n",
      "episodes 894\n",
      "exploration 0.100000\n",
      "Timestep 1100000\n",
      "mean reward (100 episodes) -14.100000\n",
      "best mean reward -14.100000\n",
      "episodes 898\n",
      "exploration 0.100000\n",
      "Timestep 1110000\n",
      "mean reward (100 episodes) -14.140000\n",
      "best mean reward -14.100000\n",
      "episodes 902\n",
      "exploration 0.100000\n",
      "Timestep 1120000\n",
      "mean reward (100 episodes) -14.040000\n",
      "best mean reward -14.000000\n",
      "episodes 906\n",
      "exploration 0.100000\n",
      "Timestep 1130000\n",
      "mean reward (100 episodes) -14.000000\n",
      "best mean reward -14.000000\n",
      "episodes 910\n",
      "exploration 0.100000\n",
      "Timestep 1140000\n",
      "mean reward (100 episodes) -13.910000\n",
      "best mean reward -13.910000\n",
      "episodes 913\n",
      "exploration 0.100000\n",
      "Timestep 1150000\n",
      "mean reward (100 episodes) -13.790000\n",
      "best mean reward -13.790000\n",
      "episodes 917\n",
      "exploration 0.100000\n",
      "Timestep 1160000\n",
      "mean reward (100 episodes) -13.700000\n",
      "best mean reward -13.650000\n",
      "episodes 921\n",
      "exploration 0.100000\n",
      "Timestep 1170000\n",
      "mean reward (100 episodes) -13.530000\n",
      "best mean reward -13.530000\n",
      "episodes 924\n",
      "exploration 0.100000\n",
      "Timestep 1180000\n",
      "mean reward (100 episodes) -13.380000\n",
      "best mean reward -13.340000\n",
      "episodes 928\n",
      "exploration 0.100000\n",
      "Timestep 1190000\n",
      "mean reward (100 episodes) -13.270000\n",
      "best mean reward -13.200000\n",
      "episodes 933\n",
      "exploration 0.100000\n",
      "Timestep 1200000\n",
      "mean reward (100 episodes) -13.250000\n",
      "best mean reward -13.200000\n",
      "episodes 937\n",
      "exploration 0.100000\n",
      "Timestep 1210000\n",
      "mean reward (100 episodes) -13.090000\n",
      "best mean reward -13.090000\n",
      "episodes 941\n",
      "exploration 0.100000\n",
      "Timestep 1220000\n",
      "mean reward (100 episodes) -13.000000\n",
      "best mean reward -13.000000\n",
      "episodes 945\n",
      "exploration 0.100000\n",
      "Timestep 1230000\n",
      "mean reward (100 episodes) -13.020000\n",
      "best mean reward -12.990000\n",
      "episodes 950\n",
      "exploration 0.100000\n",
      "Timestep 1240000\n",
      "mean reward (100 episodes) -12.970000\n",
      "best mean reward -12.940000\n",
      "episodes 954\n",
      "exploration 0.100000\n",
      "Timestep 1250000\n",
      "mean reward (100 episodes) -12.990000\n",
      "best mean reward -12.910000\n",
      "episodes 959\n",
      "exploration 0.100000\n",
      "Timestep 1260000\n",
      "mean reward (100 episodes) -12.910000\n",
      "best mean reward -12.870000\n",
      "episodes 963\n",
      "exploration 0.100000\n",
      "Timestep 1270000\n",
      "mean reward (100 episodes) -12.820000\n",
      "best mean reward -12.820000\n",
      "episodes 967\n",
      "exploration 0.100000\n",
      "Timestep 1280000\n",
      "mean reward (100 episodes) -12.670000\n",
      "best mean reward -12.670000\n",
      "episodes 970\n",
      "exploration 0.100000\n",
      "Timestep 1290000\n",
      "mean reward (100 episodes) -12.760000\n",
      "best mean reward -12.670000\n",
      "episodes 974\n",
      "exploration 0.100000\n",
      "Timestep 1300000\n",
      "mean reward (100 episodes) -12.610000\n",
      "best mean reward -12.610000\n",
      "episodes 978\n",
      "exploration 0.100000\n",
      "Timestep 1310000\n",
      "mean reward (100 episodes) -12.640000\n",
      "best mean reward -12.590000\n",
      "episodes 982\n",
      "exploration 0.100000\n",
      "Timestep 1320000\n",
      "mean reward (100 episodes) -12.380000\n",
      "best mean reward -12.380000\n",
      "episodes 986\n",
      "exploration 0.100000\n",
      "Timestep 1330000\n",
      "mean reward (100 episodes) -12.240000\n",
      "best mean reward -12.240000\n",
      "episodes 990\n",
      "exploration 0.100000\n",
      "Timestep 1340000\n",
      "mean reward (100 episodes) -12.090000\n",
      "best mean reward -12.090000\n",
      "episodes 994\n",
      "exploration 0.100000\n",
      "Timestep 1350000\n",
      "mean reward (100 episodes) -12.030000\n",
      "best mean reward -11.910000\n",
      "episodes 998\n",
      "exploration 0.100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-07-14 19:37:01,027] Starting new video recorder writing to /tmp/summerschool/user10/lab5/results/pong/openaigym.video.0.4125.video001000.mp4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 1360000\n",
      "mean reward (100 episodes) -11.680000\n",
      "best mean reward -11.680000\n",
      "episodes 1002\n",
      "exploration 0.100000\n",
      "Timestep 1370000\n",
      "mean reward (100 episodes) -11.550000\n",
      "best mean reward -11.550000\n",
      "episodes 1005\n",
      "exploration 0.100000\n",
      "Timestep 1380000\n",
      "mean reward (100 episodes) -11.630000\n",
      "best mean reward -11.550000\n",
      "episodes 1009\n",
      "exploration 0.100000\n",
      "Timestep 1390000\n",
      "mean reward (100 episodes) -11.710000\n",
      "best mean reward -11.550000\n",
      "episodes 1014\n",
      "exploration 0.100000\n",
      "Timestep 1400000\n",
      "mean reward (100 episodes) -11.630000\n",
      "best mean reward -11.550000\n",
      "episodes 1018\n",
      "exploration 0.100000\n",
      "Timestep 1410000\n",
      "mean reward (100 episodes) -11.380000\n",
      "best mean reward -11.380000\n",
      "episodes 1021\n",
      "exploration 0.100000\n",
      "Timestep 1420000\n",
      "mean reward (100 episodes) -11.330000\n",
      "best mean reward -11.280000\n",
      "episodes 1024\n",
      "exploration 0.100000\n",
      "Timestep 1430000\n",
      "mean reward (100 episodes) -11.110000\n",
      "best mean reward -11.110000\n",
      "episodes 1028\n",
      "exploration 0.100000\n",
      "Timestep 1440000\n",
      "mean reward (100 episodes) -11.000000\n",
      "best mean reward -10.990000\n",
      "episodes 1031\n",
      "exploration 0.100000\n",
      "Timestep 1450000\n",
      "mean reward (100 episodes) -10.700000\n",
      "best mean reward -10.700000\n",
      "episodes 1035\n",
      "exploration 0.100000\n",
      "Timestep 1460000\n",
      "mean reward (100 episodes) -10.710000\n",
      "best mean reward -10.700000\n",
      "episodes 1038\n",
      "exploration 0.100000\n",
      "Timestep 1470000\n",
      "mean reward (100 episodes) -10.600000\n",
      "best mean reward -10.600000\n",
      "episodes 1042\n",
      "exploration 0.100000\n",
      "Timestep 1480000\n",
      "mean reward (100 episodes) -10.330000\n",
      "best mean reward -10.330000\n",
      "episodes 1045\n",
      "exploration 0.100000\n",
      "Timestep 1490000\n",
      "mean reward (100 episodes) -10.150000\n",
      "best mean reward -10.150000\n",
      "episodes 1049\n",
      "exploration 0.100000\n",
      "Timestep 1500000\n",
      "mean reward (100 episodes) -10.020000\n",
      "best mean reward -10.020000\n",
      "episodes 1053\n",
      "exploration 0.100000\n",
      "Timestep 1510000\n",
      "mean reward (100 episodes) -9.860000\n",
      "best mean reward -9.860000\n",
      "episodes 1056\n",
      "exploration 0.100000\n",
      "Timestep 1520000\n",
      "mean reward (100 episodes) -9.550000\n",
      "best mean reward -9.550000\n",
      "episodes 1060\n",
      "exploration 0.100000\n",
      "Timestep 1530000\n",
      "mean reward (100 episodes) -9.420000\n",
      "best mean reward -9.420000\n",
      "episodes 1063\n",
      "exploration 0.100000\n",
      "Timestep 1540000\n",
      "mean reward (100 episodes) -9.330000\n",
      "best mean reward -9.330000\n",
      "episodes 1067\n",
      "exploration 0.100000\n",
      "Timestep 1550000\n",
      "mean reward (100 episodes) -9.480000\n",
      "best mean reward -9.330000\n",
      "episodes 1071\n",
      "exploration 0.100000\n",
      "Timestep 1560000\n",
      "mean reward (100 episodes) -9.470000\n",
      "best mean reward -9.330000\n",
      "episodes 1075\n",
      "exploration 0.100000\n",
      "Timestep 1570000\n",
      "mean reward (100 episodes) -9.490000\n",
      "best mean reward -9.330000\n",
      "episodes 1079\n",
      "exploration 0.100000\n",
      "Timestep 1580000\n",
      "mean reward (100 episodes) -9.320000\n",
      "best mean reward -9.320000\n",
      "episodes 1082\n",
      "exploration 0.100000\n",
      "Timestep 1590000\n",
      "mean reward (100 episodes) -9.320000\n",
      "best mean reward -9.240000\n",
      "episodes 1086\n",
      "exploration 0.100000\n",
      "Timestep 1600000\n",
      "mean reward (100 episodes) -9.130000\n",
      "best mean reward -9.130000\n",
      "episodes 1089\n",
      "exploration 0.100000\n",
      "Timestep 1610000\n",
      "mean reward (100 episodes) -9.040000\n",
      "best mean reward -9.020000\n",
      "episodes 1093\n",
      "exploration 0.100000\n",
      "Timestep 1620000\n",
      "mean reward (100 episodes) -8.960000\n",
      "best mean reward -8.960000\n",
      "episodes 1096\n",
      "exploration 0.100000\n",
      "Timestep 1630000\n",
      "mean reward (100 episodes) -8.850000\n",
      "best mean reward -8.780000\n",
      "episodes 1099\n",
      "exploration 0.100000\n",
      "Timestep 1640000\n",
      "mean reward (100 episodes) -8.760000\n",
      "best mean reward -8.660000\n",
      "episodes 1102\n",
      "exploration 0.100000\n",
      "Timestep 1650000\n",
      "mean reward (100 episodes) -8.700000\n",
      "best mean reward -8.660000\n",
      "episodes 1105\n",
      "exploration 0.100000\n",
      "Timestep 1660000\n",
      "mean reward (100 episodes) -8.280000\n",
      "best mean reward -8.280000\n",
      "episodes 1108\n",
      "exploration 0.100000\n",
      "Timestep 1670000\n",
      "mean reward (100 episodes) -8.150000\n",
      "best mean reward -8.150000\n",
      "episodes 1111\n",
      "exploration 0.100000\n",
      "Timestep 1680000\n",
      "mean reward (100 episodes) -7.910000\n",
      "best mean reward -7.910000\n",
      "episodes 1114\n",
      "exploration 0.100000\n",
      "Timestep 1690000\n",
      "mean reward (100 episodes) -7.620000\n",
      "best mean reward -7.620000\n",
      "episodes 1117\n",
      "exploration 0.100000\n",
      "Timestep 1700000\n",
      "mean reward (100 episodes) -7.730000\n",
      "best mean reward -7.590000\n",
      "episodes 1121\n",
      "exploration 0.100000\n",
      "Timestep 1710000\n",
      "mean reward (100 episodes) -7.760000\n",
      "best mean reward -7.590000\n",
      "episodes 1124\n",
      "exploration 0.100000\n",
      "Timestep 1720000\n",
      "mean reward (100 episodes) -7.820000\n",
      "best mean reward -7.590000\n",
      "episodes 1127\n",
      "exploration 0.100000\n",
      "Timestep 1730000\n",
      "mean reward (100 episodes) -7.900000\n",
      "best mean reward -7.590000\n",
      "episodes 1130\n",
      "exploration 0.100000\n",
      "Timestep 1740000\n",
      "mean reward (100 episodes) -7.780000\n",
      "best mean reward -7.590000\n",
      "episodes 1132\n",
      "exploration 0.100000\n",
      "Timestep 1750000\n",
      "mean reward (100 episodes) -7.600000\n",
      "best mean reward -7.590000\n",
      "episodes 1135\n",
      "exploration 0.100000\n",
      "Timestep 1760000\n",
      "mean reward (100 episodes) -7.210000\n",
      "best mean reward -7.210000\n",
      "episodes 1138\n",
      "exploration 0.100000\n",
      "Timestep 1770000\n",
      "mean reward (100 episodes) -6.950000\n",
      "best mean reward -6.950000\n",
      "episodes 1141\n",
      "exploration 0.100000\n",
      "Timestep 1780000\n",
      "mean reward (100 episodes) -7.020000\n",
      "best mean reward -6.890000\n",
      "episodes 1144\n",
      "exploration 0.100000\n",
      "Timestep 1790000\n",
      "mean reward (100 episodes) -6.800000\n",
      "best mean reward -6.800000\n",
      "episodes 1147\n",
      "exploration 0.100000\n",
      "Timestep 1800000\n",
      "mean reward (100 episodes) -6.530000\n",
      "best mean reward -6.530000\n",
      "episodes 1150\n",
      "exploration 0.100000\n",
      "Timestep 1810000\n",
      "mean reward (100 episodes) -6.420000\n",
      "best mean reward -6.420000\n",
      "episodes 1153\n",
      "exploration 0.100000\n",
      "Timestep 1820000\n",
      "mean reward (100 episodes) -6.300000\n",
      "best mean reward -6.290000\n",
      "episodes 1156\n",
      "exploration 0.100000\n",
      "Timestep 1830000\n",
      "mean reward (100 episodes) -6.240000\n",
      "best mean reward -6.200000\n",
      "episodes 1159\n",
      "exploration 0.100000\n",
      "Timestep 1840000\n",
      "mean reward (100 episodes) -5.970000\n",
      "best mean reward -5.970000\n",
      "episodes 1161\n",
      "exploration 0.100000\n",
      "Timestep 1850000\n",
      "mean reward (100 episodes) -5.750000\n",
      "best mean reward -5.750000\n",
      "episodes 1164\n",
      "exploration 0.100000\n",
      "Timestep 1860000\n",
      "mean reward (100 episodes) -5.430000\n",
      "best mean reward -5.430000\n",
      "episodes 1167\n",
      "exploration 0.100000\n",
      "Timestep 1870000\n",
      "mean reward (100 episodes) -5.270000\n",
      "best mean reward -5.270000\n",
      "episodes 1169\n",
      "exploration 0.100000\n",
      "Timestep 1880000\n",
      "mean reward (100 episodes) -4.880000\n",
      "best mean reward -4.880000\n",
      "episodes 1172\n",
      "exploration 0.100000\n",
      "Timestep 1890000\n",
      "mean reward (100 episodes) -4.820000\n",
      "best mean reward -4.820000\n",
      "episodes 1174\n",
      "exploration 0.100000\n",
      "Timestep 1900000\n",
      "mean reward (100 episodes) -4.380000\n",
      "best mean reward -4.380000\n",
      "episodes 1177\n",
      "exploration 0.100000\n",
      "Timestep 1910000\n",
      "mean reward (100 episodes) -4.130000\n",
      "best mean reward -4.130000\n",
      "episodes 1179\n",
      "exploration 0.100000\n",
      "Timestep 1920000\n",
      "mean reward (100 episodes) -3.860000\n",
      "best mean reward -3.860000\n",
      "episodes 1182\n",
      "exploration 0.100000\n",
      "Timestep 1930000\n",
      "mean reward (100 episodes) -3.850000\n",
      "best mean reward -3.850000\n",
      "episodes 1184\n",
      "exploration 0.100000\n",
      "Timestep 1940000\n",
      "mean reward (100 episodes) -3.580000\n",
      "best mean reward -3.580000\n",
      "episodes 1187\n",
      "exploration 0.100000\n",
      "Timestep 1950000\n",
      "mean reward (100 episodes) -3.450000\n",
      "best mean reward -3.450000\n",
      "episodes 1189\n",
      "exploration 0.100000\n",
      "Timestep 1960000\n",
      "mean reward (100 episodes) -3.300000\n",
      "best mean reward -3.300000\n",
      "episodes 1191\n",
      "exploration 0.100000\n",
      "Timestep 1970000\n",
      "mean reward (100 episodes) -3.060000\n",
      "best mean reward -3.060000\n",
      "episodes 1193\n",
      "exploration 0.100000\n",
      "Timestep 1980000\n",
      "mean reward (100 episodes) -2.890000\n",
      "best mean reward -2.890000\n",
      "episodes 1195\n",
      "exploration 0.100000\n",
      "Timestep 1990000\n",
      "mean reward (100 episodes) -2.740000\n",
      "best mean reward -2.740000\n",
      "episodes 1198\n",
      "exploration 0.100000\n",
      "Timestep 2000000\n",
      "mean reward (100 episodes) -2.460000\n",
      "best mean reward -2.460000\n",
      "episodes 1200\n",
      "exploration 0.100000\n",
      "Timestep 2010000\n",
      "mean reward (100 episodes) -2.260000\n",
      "best mean reward -2.260000\n",
      "episodes 1203\n",
      "exploration 0.100000\n",
      "Timestep 2020000\n",
      "mean reward (100 episodes) -2.350000\n",
      "best mean reward -2.260000\n",
      "episodes 1205\n",
      "exploration 0.100000\n",
      "Timestep 2030000\n",
      "mean reward (100 episodes) -2.370000\n",
      "best mean reward -2.260000\n",
      "episodes 1208\n",
      "exploration 0.100000\n",
      "Timestep 2040000\n",
      "mean reward (100 episodes) -2.190000\n",
      "best mean reward -2.190000\n",
      "episodes 1210\n",
      "exploration 0.100000\n",
      "Timestep 2050000\n",
      "mean reward (100 episodes) -2.190000\n",
      "best mean reward -2.190000\n",
      "episodes 1212\n",
      "exploration 0.100000\n",
      "Timestep 2060000\n",
      "mean reward (100 episodes) -2.000000\n",
      "best mean reward -2.000000\n",
      "episodes 1214\n",
      "exploration 0.100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestep 2070000\n",
      "mean reward (100 episodes) -1.840000\n",
      "best mean reward -1.840000\n",
      "episodes 1217\n",
      "exploration 0.100000\n",
      "Timestep 2080000\n",
      "mean reward (100 episodes) -1.570000\n",
      "best mean reward -1.570000\n",
      "episodes 1219\n",
      "exploration 0.100000\n",
      "Timestep 2090000\n",
      "mean reward (100 episodes) -1.230000\n",
      "best mean reward -1.230000\n",
      "episodes 1222\n",
      "exploration 0.100000\n",
      "Timestep 2100000\n",
      "mean reward (100 episodes) -1.040000\n",
      "best mean reward -1.040000\n",
      "episodes 1224\n",
      "exploration 0.100000\n",
      "Timestep 2110000\n",
      "mean reward (100 episodes) -0.640000\n",
      "best mean reward -0.640000\n",
      "episodes 1227\n",
      "exploration 0.100000\n",
      "Timestep 2120000\n",
      "mean reward (100 episodes) -0.510000\n",
      "best mean reward -0.510000\n",
      "episodes 1230\n",
      "exploration 0.100000\n",
      "Timestep 2130000\n",
      "mean reward (100 episodes) -0.490000\n",
      "best mean reward -0.440000\n",
      "episodes 1233\n",
      "exploration 0.100000\n",
      "Timestep 2140000\n",
      "mean reward (100 episodes) -0.370000\n",
      "best mean reward -0.370000\n",
      "episodes 1235\n",
      "exploration 0.100000\n",
      "Timestep 2150000\n",
      "mean reward (100 episodes) -0.300000\n",
      "best mean reward -0.240000\n",
      "episodes 1237\n",
      "exploration 0.100000\n",
      "Timestep 2160000\n",
      "mean reward (100 episodes) -0.280000\n",
      "best mean reward -0.240000\n",
      "episodes 1239\n",
      "exploration 0.100000\n",
      "Timestep 2170000\n",
      "mean reward (100 episodes) -0.310000\n",
      "best mean reward -0.240000\n",
      "episodes 1242\n",
      "exploration 0.100000\n",
      "Timestep 2180000\n",
      "mean reward (100 episodes) -0.100000\n",
      "best mean reward -0.020000\n",
      "episodes 1245\n",
      "exploration 0.100000\n",
      "Timestep 2190000\n",
      "mean reward (100 episodes) 0.110000\n",
      "best mean reward 0.110000\n",
      "episodes 1247\n",
      "exploration 0.100000\n",
      "Timestep 2200000\n",
      "mean reward (100 episodes) 0.200000\n",
      "best mean reward 0.200000\n",
      "episodes 1250\n",
      "exploration 0.100000\n",
      "Timestep 2210000\n",
      "mean reward (100 episodes) 0.150000\n",
      "best mean reward 0.200000\n",
      "episodes 1252\n",
      "exploration 0.100000\n",
      "Timestep 2220000\n",
      "mean reward (100 episodes) 0.300000\n",
      "best mean reward 0.340000\n",
      "episodes 1255\n",
      "exploration 0.100000\n",
      "Timestep 2230000\n",
      "mean reward (100 episodes) 0.640000\n",
      "best mean reward 0.640000\n",
      "episodes 1258\n",
      "exploration 0.100000\n",
      "Timestep 2240000\n",
      "mean reward (100 episodes) 0.540000\n",
      "best mean reward 0.640000\n",
      "episodes 1261\n",
      "exploration 0.100000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-37a9b1fc40d0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mlearning_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_FREQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mframe_history_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFRAME_HISTORY_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mtarget_update_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTARGET_UPDATE_FREQ\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     )\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7f9f1ade5f72>\u001b[0m in \u001b[0;36mdqn_learing\u001b[0;34m(env, q_func, optimizer_spec, exploration, stopping_criterion, replay_buffer_size, batch_size, gamma, learning_starts, learning_freq, frame_history_len, target_update_freq)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Choose random action if not yet start learning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mlearning_starts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_epilson_greedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecent_observations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7f9f1ade5f72>\u001b[0m in \u001b[0;36mselect_epilson_greedy_action\u001b[0;34m(model, obs, t)\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0meps_threshold\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexploration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0meps_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m255.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m             \u001b[0;31m# Use volatile = True if variable is only used in inference mode, i.e. don’t save the history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Libraries/miniconda3/lib/python3.6/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36m_type\u001b[0;34m(self, new_type, async)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot cast dense tensor to sparse tensor\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##########\n",
    "## MAIN ##\n",
    "##########\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    ## Set Gym\n",
    "    # Get Atari games.\n",
    "    atari = gym.benchmark_spec('Atari40M')\n",
    "\n",
    "    # Change the index to select a different game.\n",
    "    task = atari.tasks[PONG]\n",
    "    num_timesteps = task.max_timesteps\n",
    "    \n",
    "    ## Run training\n",
    "    seed = 0        # Use a seed of zero\n",
    "    env = get_env(task, seed)\n",
    "    \n",
    "    def stopping_criterion(env):\n",
    "        # notice that here t is the number of steps of the wrapped env,\n",
    "        # which is different from the number of steps in the underlying env\n",
    "        return get_wrapper_by_name(env, \"Monitor\").get_total_steps() >= num_timesteps\n",
    "    \n",
    "    optimizer_spec = OptimizerSpec(\n",
    "        constructor=optim.RMSprop,\n",
    "        kwargs=dict(lr=LEARNING_RATE, alpha=ALPHA, eps=EPS),\n",
    "    )\n",
    "    \n",
    "    exploration_schedule = LinearSchedule(1000000, 0.1)\n",
    "    \n",
    "    dqn_learing(\n",
    "        env=env,\n",
    "        q_func=DQN,\n",
    "        optimizer_spec=optimizer_spec,\n",
    "        exploration=exploration_schedule,\n",
    "        stopping_criterion=stopping_criterion,\n",
    "        replay_buffer_size=REPLAY_BUFFER_SIZE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        gamma=GAMMA,\n",
    "        learning_starts=LEARNING_STARTS,\n",
    "        learning_freq=LEARNING_FREQ,\n",
    "        frame_history_len=FRAME_HISTORY_LEN,\n",
    "        target_update_freq=TARGET_UPDATE_FREQ,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Statistic[\"mean_episode_rewards\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(Statistic[\"best_mean_episode_rewards\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Excercise\n",
    "\n",
    "0. Observe the performance of the DQN at various training phases in the results folder.\n",
    "1. Change the hyperparameters, and observe the performance of DQN.\n",
    "2. Use different `Schedules` (`Constant`, `Piecewise`) and compare them with `LinearSchedule`. (Codes for these are already included, you just need to replace `LinearSchedule` with them.\n",
    "3. Apply different exploration strategies like `greedy`, `random` and check perfromance of the DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement\n",
    "\n",
    "The helper functions have been adapted/copied from Berkely deep learning course [homework 3](https://github.com/berkeleydeeprlcourse/homework/tree/master/hw3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
